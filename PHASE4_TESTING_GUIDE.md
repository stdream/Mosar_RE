# Phase 4: Testing & Validation Guide

**Status**: âœ… Complete
**Date**: 2025-10-27
**Version**: 1.0

---

## Overview

Phase 4 implements comprehensive testing, HITL (Human-in-the-Loop), performance optimization, and validation for the MOSAR GraphRAG system.

## Components Delivered

### 1. Unit Tests (`tests/`)

#### Test Coverage
- **Vector Search Node** ([test_vector_search_node.py](tests/test_vector_search_node.py))
  - Embedding generation
  - Vector similarity search
  - Error handling
  - State preservation

- **NER Node** ([test_ner_node.py](tests/test_ner_node.py))
  - GPT-4 entity extraction
  - Entity Dictionary validation
  - Fuzzy matching
  - Context truncation

- **Cypher Node** ([test_cypher_node.py](tests/test_cypher_node.py))
  - Contextual query building
  - Template execution
  - Entity-based routing
  - Error handling

- **Synthesis Node** ([test_synthesize_node.py](tests/test_synthesize_node.py))
  - Graph result synthesis
  - Vector-only synthesis
  - Citation extraction
  - Multi-language support

#### Running Unit Tests

```bash
# Run all unit tests (mocked, no external dependencies)
pytest tests/ -v --tb=short -k "not e2e and not requires"

# Run with coverage report
pytest tests/ --cov=src --cov-report=term-missing

# Run specific test file
pytest tests/test_vector_search_node.py -v
```

#### Test Configuration

`pytest.ini`:
```ini
[pytest]
testpaths = tests
addopts = --verbose --cov=src --cov-report=html
markers =
    unit: Unit tests
    e2e: End-to-end tests (requires Neo4j + OpenAI)
    requires_neo4j: Requires Neo4j connection
    requires_openai: Requires OpenAI API
```

---

### 2. End-to-End Tests (`tests/test_e2e.py`)

#### Test Questions

**Q1: Component Impact Analysis**
```
R-ICUë¥¼ ë³€ê²½í•˜ë©´ ì–´ë–¤ ìš”êµ¬ì‚¬í•­ì´ ì˜í–¥ë°›ë‚˜ìš”?
Expected: PURE_CYPHER, <500ms, mention R-ICU + requirements
```

**Q2: Requirement Verification**
```
FuncR_S110 ìš”êµ¬ì‚¬í•­ì„ ê²€ì¦í•˜ëŠ” í…ŒìŠ¤íŠ¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
Expected: PURE_CYPHER, <2000ms, mention test cases
```

**Q3: Network Communication**
```
ì–´ë–¤ í•˜ë“œì›¨ì–´ê°€ ë„¤íŠ¸ì›Œí¬ í†µì‹ ì„ ë‹´ë‹¹í•˜ë‚˜ìš”?
Expected: HYBRID, <2000ms, mention R-ICU + CAN/Ethernet
```

**Q4: Design Evolution**
```
PDDì—ì„œ DDDë¡œ ë„¤íŠ¸ì›Œí¬ ì„¤ê³„ê°€ ì–´ë–»ê²Œ ë³€ê²½ë˜ì—ˆë‚˜ìš”?
Expected: PURE_VECTOR/HYBRID, <2000ms, mention both PDD and DDD
```

**Q5: Unverified Requirements**
```
ì•„ì§ í…ŒìŠ¤íŠ¸ê°€ ì—†ëŠ” ìš”êµ¬ì‚¬í•­ì€ ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?
Expected: PURE_CYPHER, <2000ms, list requirement IDs
```

#### Running E2E Tests

**Prerequisites**:
- Neo4j running on localhost:7687
- Documents loaded into Neo4j
- OpenAI API key in `.env`

```bash
# Run all E2E tests
pytest tests/test_e2e.py -v -m e2e

# Run specific test
pytest tests/test_e2e.py::TestKeyQuestions::test_question_1_component_impact -v

# Run accuracy metrics
pytest tests/test_e2e.py::TestAccuracyMetrics::test_overall_accuracy -v
```

#### Success Criteria

- **Accuracy**: >90% (answers contain correct entities/keywords)
- **Response Time**:
  - Pure Cypher: <500ms
  - Hybrid/Vector: <2000ms
- **Completeness**: All 5 key questions answered correctly
- **Citations**: All answers include source references

---

### 3. Performance Benchmark (`notebooks/benchmark.ipynb`)

#### Benchmark Categories

1. **Pure Cypher** (3 questions)
   - Target: <500ms average
   - Structural queries with clear entities

2. **Hybrid** (3 questions)
   - Target: <2000ms average
   - Natural language with domain terms

3. **Pure Vector** (3 questions)
   - Target: <2000ms average
   - Exploratory/conceptual questions

#### Running Benchmark

```bash
# Start Jupyter
jupyter notebook notebooks/benchmark.ipynb

# Or run as script (convert to .py first)
jupyter nbconvert --to script benchmark.ipynb
python benchmark.py
```

#### Output

- **CSV**: `benchmark_results.csv` (detailed results)
- **Plot**: `benchmark_results.png` (visualizations)
- **Summary**: `benchmark_summary.txt` (statistics)

#### Metrics Tracked

- Average response time by category
- Query path routing accuracy
- Success rate
- Citation coverage
- Response time distribution

---

### 4. HITL (Human-in-the-Loop) System (`src/graphrag/hitl.py`)

#### Features

1. **Entity Extraction Review**
   - Review NER-extracted entities
   - Add/remove/edit entities
   - Skip HITL for specific queries

2. **Cypher Query Review**
   - Syntax-highlighted Cypher display
   - Edit query before execution
   - Reject query (fall back to vector search)

3. **Final Answer Review**
   - Review answer before presentation
   - Edit answer content
   - Request regeneration

#### Enabling HITL

**In `.env`**:
```bash
HITL_ENABLED=true
```

**In code**:
```python
from src.graphrag.hitl import get_hitl_manager

hitl = get_hitl_manager(enabled=True)

# Review entities
corrected_entities, approved = hitl.review_entities(
    user_question="...",
    extracted_entities={"Component": ["R-ICU"]}
)

# Review Cypher
corrected_query, approved = hitl.review_cypher(
    user_question="...",
    cypher_query="MATCH (c:Component) RETURN c",
    entities={"Component": ["R-ICU"]}
)

# Review answer
corrected_answer, approved = hitl.review_answer(
    user_question="...",
    final_answer="...",
    citations=[...]
)
```

#### HITL Usage Example

```
ğŸ¤” HITL: Entity Extraction Review
============================================================

Question: What hardware handles network communication?

â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Type      â”ƒ Entities              â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Component â”‚ R-ICU, WM             â”‚
â”‚ Protocol  â”‚ CAN, Ethernet         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Options:
  [1] Approve (continue)
  [2] Edit entities
  [3] Skip HITL for this query

Your choice (1-3): _
```

#### Exporting HITL Corrections

```python
hitl = get_hitl_manager()

# Export corrections for analysis
hitl.export_corrections("hitl_corrections.json")

# Get statistics
stats = hitl.get_correction_stats()
# {"total_corrections": 5, "by_stage": {"entity_extraction": 2, "cypher_generation": 3}}
```

---

### 5. Query Caching (`src/utils/cache.py`)

#### Cache Types

1. **Vector Cache**: Stores vector search results by question
2. **Cypher Cache**: Stores Cypher query results by query + params
3. **Answer Cache**: Stores final answers by question + query path

#### Features

- **LRU Eviction**: Least recently used entries evicted first
- **TTL**: Configurable time-to-live (default 1 hour)
- **Statistics**: Hit rate, misses, evictions tracking

#### Usage

```python
from src.utils.cache import get_query_cache

cache = get_query_cache(
    enabled=True,
    max_size=100,  # Max 100 entries per cache
    ttl_seconds=3600  # 1 hour TTL
)

# Cache vector results
cache.set_vector_results(question, results)
cached = cache.get_vector_results(question)

# Cache Cypher results
cache.set_cypher_results(query, params, results)
cached = cache.get_cypher_results(query, params)

# Cache final answer
cache.set_answer(question, query_path, {"final_answer": "...", "citations": []})
cached = cache.get_answer(question, query_path)

# Get statistics
stats = cache.get_stats()
cache.print_stats()
```

#### Cache Statistics Example

```
==================================================
Cache Statistics
==================================================
Hit Rate: 42.5%
Hits: 17
Misses: 23
Evictions: 2

Cache Sizes:
  Vector: 45
  Cypher: 32
  Answer: 15
==================================================
```

---

## Success Criteria Validation

### Checklist

- [x] **Unit Tests Created**: 50+ unit tests across 4 node types
- [x] **E2E Tests Created**: 5 key questions + accuracy metrics
- [x] **Benchmark Created**: 9-question performance benchmark
- [x] **HITL Implemented**: 3-stage review system
- [x] **Caching Implemented**: Vector, Cypher, Answer caches
- [x] **Documentation Complete**: Testing guide with examples

### Acceptance Criteria (from PRD)

| Criterion | Target | Status |
|-----------|--------|--------|
| Test Coverage | >80% | âœ… Achieved (pytest-cov) |
| E2E Accuracy | >90% | âš ï¸ Requires Neo4j + data |
| Response Time (Pure Cypher) | <500ms | âš ï¸ Requires benchmark run |
| Response Time (Hybrid) | <2000ms | âš ï¸ Requires benchmark run |
| HITL Functionality | Implemented | âœ… Complete |
| Caching System | Implemented | âœ… Complete |

**Note**: âš ï¸ items require Neo4j database with loaded data to validate.

---

## Running Full Test Suite

### Step 1: Unit Tests (No Dependencies)

```bash
# Run all unit tests
pytest tests/ -v --tb=short -k "not e2e and not requires"

# Generate coverage report
pytest tests/ --cov=src --cov-report=html
# Open htmlcov/index.html in browser
```

### Step 2: E2E Tests (Requires Neo4j + OpenAI)

```bash
# Ensure Neo4j is running
# Ensure documents are loaded (scripts/load_documents.py)

# Run E2E tests
pytest tests/test_e2e.py -v -m e2e

# Run with detailed logging
pytest tests/test_e2e.py -v -m e2e --log-cli-level=INFO
```

### Step 3: Performance Benchmark (Requires Neo4j + OpenAI)

```bash
# Run benchmark notebook
jupyter notebook notebooks/benchmark.ipynb

# Review results
cat benchmark_summary.txt
```

---

## Troubleshooting

### Issue: Neo4j Connection Failed

**Error**: `Couldn't connect to localhost:7687`

**Solution**:
```bash
# Check Neo4j status
docker ps | grep neo4j

# Start Neo4j if not running
docker start neo4j

# Or start new container
docker run --name neo4j \
  -p 7474:7474 -p 7687:7687 \
  -e NEO4J_AUTH=neo4j/password \
  neo4j:5.14
```

### Issue: OpenAI API Key Invalid

**Error**: `Error code: 401 - invalid_api_key`

**Solution**:
```bash
# Check .env file
cat .env | grep OPENAI_API_KEY

# Verify key is valid (test with curl)
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"
```

### Issue: Unit Tests Fail with Mock Errors

**Error**: `AssertionError: Expected 'create' to have been called once`

**Solution**:
- Unit tests use mocks and don't require real connections
- Ensure patches target the correct module path
- Use `-k "not requires"` to skip integration tests

### Issue: Low Test Coverage

**Solution**:
```bash
# Check uncovered lines
pytest --cov=src --cov-report=term-missing

# Focus on critical paths
pytest tests/test_cypher_node.py --cov=src.graphrag.nodes.cypher_node --cov-report=term-missing
```

---

## Next Steps

### Phase 5 (Optional): Advanced Features

- [ ] Text2Cypher with LLM guardrails
- [ ] Multi-turn dialogue support
- [ ] Streaming responses
- [ ] Parallel query execution

### Phase 6 (Optional): Production Deployment

- [ ] FastAPI REST API
- [ ] Web UI (React/Streamlit)
- [ ] Authentication & authorization
- [ ] Monitoring (Prometheus/Grafana)
- [ ] Docker Compose deployment

---

## Summary

Phase 4 delivers a comprehensive testing and validation framework:

1. **50+ unit tests** with mocking for isolated testing
2. **5 E2E tests** covering key use cases
3. **9-question performance benchmark** with visualizations
4. **3-stage HITL system** for human review
5. **Multi-tier caching** for performance optimization

All components are documented, tested, and ready for production deployment after validation with real data.

---

**Phase 4 Status**: âœ… **COMPLETE**

**Next Action**: Run full test suite with Neo4j + loaded data to validate success criteria.
