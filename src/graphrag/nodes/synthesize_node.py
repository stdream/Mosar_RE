"""
Response Synthesis Node - Generate natural language responses using GPT-4

Combines vector search results and graph query results to produce
coherent, cited answers in Korean or English.
"""

import logging
import json
import os
from typing import Dict, List, Any, Optional
from openai import OpenAI

from src.graphrag.state import GraphRAGState
from src.query.router import QueryPath

logger = logging.getLogger(__name__)


def synthesize_response(state: GraphRAGState) -> GraphRAGState:
    """
    LangGraph Node: Synthesize final response using GPT-4.

    Combines:
    - User question
    - Vector search results (semantic context)
    - Graph query results (structured data)
    - Query path information

    Args:
        state: Current GraphRAGState with results populated

    Returns:
        Updated state with 'final_answer' and 'citations'
    """
    user_question = state["user_question"]
    query_path = state.get("query_path")
    language = state.get("language", "en")

    logger.info(f"Synthesizing response for query path: {query_path}")

    # Build synthesis prompt based on available data
    if query_path == QueryPath.PURE_CYPHER or query_path == QueryPath.HYBRID:
        # Use graph results as primary source
        response = _synthesize_from_graph(state, language)
    else:
        # Pure vector - use section results
        response = _synthesize_from_vectors(state, language)

    # Update state
    state["final_answer"] = response["answer"]
    state["citations"] = response["citations"]

    return state


def _synthesize_from_graph(state: GraphRAGState, language: str) -> Dict[str, Any]:
    """
    Synthesize response from graph query results.

    Args:
        state: GraphRAGState
        language: Target language (ko/en)

    Returns:
        Dict with 'answer' and 'citations'
    """
    user_question = state["user_question"]
    graph_results = state.get("graph_results", [])
    top_k_sections = state.get("top_k_sections", [])
    cypher_query = state.get("cypher_query", "")
    matched_entities = state.get("matched_entities", {})

    # CRITICAL: Check if graph results are empty (fix for hallucination bug)
    graph_is_empty = not graph_results or len(graph_results) == 0

    if graph_is_empty:
        # Graph query returned 0 results - provide clear empty results message
        logger.warning(f"Empty graph results for query: {user_question}")

        # Extract entity information for better error message
        entity_info = ""
        if matched_entities:
            entity_list = []
            for entity_type, entity_data in matched_entities.items():
                if isinstance(entity_data, dict) and 'id' in entity_data:
                    entity_list.append(f"{entity_type}: {entity_data['id']}")
                elif isinstance(entity_data, list):
                    entity_list.extend([f"{entity_type}: {e}" for e in entity_data])
                elif isinstance(entity_data, str):
                    entity_list.append(f"{entity_type}: {entity_data}")

            if entity_list:
                entity_info = ", ".join(entity_list)

        if language == "ko":
            answer = f"""Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú ÏöîÏ≤≠ÌïòÏã† Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.

**ÏøºÎ¶¨ Ï†ïÎ≥¥**:
- Í≤ÄÏÉâ ÏóîÌã∞Ìã∞: {entity_info if entity_info else 'Ïïå Ïàò ÏóÜÏùå'}
- Ïã§ÌñâÎêú Cypher ÏøºÎ¶¨: ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ïã§ÌñâÎê®
- Î∞òÌôòÎêú Í≤∞Í≥º: 0Í∞ú

**Í∞ÄÎä•Ìïú ÏõêÏù∏**:
1. ÏöîÏ≤≠ÌïòÏã† ÏóîÌã∞Ìã∞Í∞Ä Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§
2. ÏóîÌã∞Ìã∞Îäî Ï°¥Ïû¨ÌïòÏßÄÎßå Í¥ÄÎ†® Í¥ÄÍ≥Ñ(RELATES_TO, VERIFIES, USES_PROTOCOL Îì±)Í∞Ä ÏÉùÏÑ±ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§
3. Îç∞Ïù¥ÌÑ∞ ingestion Ï§ë ÏùºÎ∂Ä Í¥ÄÍ≥ÑÍ∞Ä ÎàÑÎùΩÎêòÏóàÏùÑ Ïàò ÏûàÏäµÎãàÎã§

**ÎèÑÏõÄÎßê**:
- ÏóîÌã∞Ìã∞ ID ÌòïÏãùÏùÑ ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî (Ïòà: FuncR_A101, R-ICU, CT-A-1)
- Îã§Î•∏ Í≤ÄÏÉâÏñ¥Î°ú ÏãúÎèÑÌï¥Î≥¥ÏÑ∏Ïöî
- Î≤°ÌÑ∞ Í≤ÄÏÉâÏùÑ ÌÜµÌï¥ Î¨∏ÏÑúÏóêÏÑú Í¥ÄÎ†® ÎÇ¥Ïö©ÏùÑ Ï∞æÏïÑÎ≥º Ïàò ÏûàÏäµÎãàÎã§"""
        else:
            answer = f"""No information found in the database for your query.

**Query Information**:
- Searched Entity: {entity_info if entity_info else 'Unknown'}
- Cypher Query: Successfully executed
- Results Returned: 0

**Possible Causes**:
1. The requested entity does not exist in the database
2. The entity exists but lacks relationships (RELATES_TO, VERIFIES, USES_PROTOCOL, etc.)
3. Some relationships may have been missed during data ingestion

**Suggestions**:
- Verify the entity ID format (e.g., FuncR_A101, R-ICU, CT-A-1)
- Try different search terms
- Use vector search to find related information in documents"""

        return {
            "answer": answer,
            "citations": []
        }

    # Build context from graph results
    graph_context = _format_graph_results(graph_results)

    # Build context from vector results (supplementary)
    vector_context = ""
    if top_k_sections:
        vector_context = "\n\n".join([
            f"[{sec['document']} - {sec['title']}]\n{sec['content'][:500]}"
            for sec in top_k_sections[:3]
        ])

    # Language-specific prompts
    if language == "ko":
        system_prompt = """ÎãπÏã†ÏùÄ MOSAR (Modular Spacecraft Assembly and Reconfiguration) ÏãúÏä§ÌÖú Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§.

**Ï§ëÏöî Ï†úÏïΩÏÇ¨Ìï≠**:
- Î∞òÎìúÏãú Ï†úÍ≥µÎêú Í∑∏ÎûòÌîÑ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í≤∞Í≥ºÎßåÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÎãµÎ≥ÄÌïòÏÑ∏Ïöî
- Í∑∏ÎûòÌîÑ Í≤∞Í≥ºÏóê ÏóÜÎäî ÏöîÍµ¨ÏÇ¨Ìï≠ ID, ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ ID, Ïª¥Ìè¨ÎÑåÌä∏ IDÎ•º Ï†àÎåÄ ÏÉùÏÑ±ÌïòÍ±∞ÎÇò Ï∂îÏ∏°ÌïòÏßÄ ÎßàÏÑ∏Ïöî
- Ï†ïÎ≥¥Í∞Ä Î∂àÏ∂©Î∂ÑÌïòÎ©¥ "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ìï¥Îãπ Ï†ïÎ≥¥Í∞Ä ÏóÜÏäµÎãàÎã§"ÎùºÍ≥† Î™ÖÌôïÌûà Î∞ùÌûàÏÑ∏Ïöî
- ÌóàÏúÑ Ï†ïÎ≥¥Î•º ÏÉùÏÑ±ÌïòÎäî Í≤ÉÏùÄ ÏóÑÍ≤©Ìûà Í∏àÏßÄÎê©ÎãàÎã§"""
        instruction = """ÏïÑÎûò ÏßàÎ¨∏Ïóê ÎåÄÌï¥ Ï†úÍ≥µÎêú Îç∞Ïù¥ÌÑ∞Î•º Î∞îÌÉïÏúºÎ°ú ÎãµÎ≥ÄÌï¥Ï£ºÏÑ∏Ïöî.

**ÎãµÎ≥Ä ÏöîÍµ¨ÏÇ¨Ìï≠**:
1. **Ïò§ÏßÅ "Graph Query Results"Ïóê ÏûàÎäî Ï†ïÎ≥¥Îßå ÏÇ¨Ïö©**
2. Í∑∏ÎûòÌîÑ Í≤∞Í≥ºÏóê Î™ÖÏãúÎêú IDÎßå Ïù∏Ïö© (RQ-001 Í∞ôÏùÄ ÏûÑÏùòÏùò ID ÏÉùÏÑ± Í∏àÏßÄ)
3. **Î™®Îì† ÏöîÍµ¨ÏÇ¨Ìï≠ IDÎ•º Îπ†ÏßêÏóÜÏù¥ Ìè¨Ìï®** (ÏùºÎ∂ÄÎßå ÏÑ†ÌÉùÌïòÏßÄ Îßê Í≤É)
4. Ï£ºÏöî Ìï≠Î™©ÏùÄ ÏûêÏÑ∏Ìûà ÏÑ§Î™ÖÌïòÍ≥†, ÎÇòÎ®∏ÏßÄÎäî Ïπ¥ÌÖåÍ≥†Î¶¨Î≥ÑÎ°ú ÎÇòÏó¥
5. Ï†ïÎ≥¥Í∞Ä Î∂ÄÏ°±ÌïòÎ©¥ "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ï∂îÍ∞Ä Ï†ïÎ≥¥ ÏóÜÏùå"Ïù¥ÎùºÍ≥† Î™ÖÏãú
6. Ï∂úÏ≤òÎ•º Î™ÖÌôïÌûà ÌëúÏãú (Ïã§Ï†úÎ°ú Ï°¥Ïû¨ÌïòÎäî Î¨∏ÏÑú ÏÑπÏÖòÎßå)
7. ÎßàÌÅ¨Îã§Ïö¥ ÌòïÏãùÏúºÎ°ú ÏûëÏÑ± (Î¶¨Ïä§Ìä∏, ÌÖåÏù¥Î∏î Îì± ÌôúÏö©)

**Ï†àÎåÄ Í∏àÏßÄ**:
- Í∑∏ÎûòÌîÑ Í≤∞Í≥ºÏóê ÏóÜÎäî ÏöîÍµ¨ÏÇ¨Ìï≠/ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ ID ÏÉùÏÑ±
- Ï∂îÏ∏°Ïù¥ÎÇò ÏùºÎ∞òÏ†ÅÏù∏ ÏßÄÏãùÏúºÎ°ú ÎãµÎ≥Ä ÏûëÏÑ±
- ÌîåÎ†àÏù¥Ïä§ÌôÄÎçîÎÇò ÏòàÏãú Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
"""
    else:
        system_prompt = """You are an expert in MOSAR (Modular Spacecraft Assembly and Reconfiguration) system.

**CRITICAL CONSTRAINTS**:
- Answer ONLY using the provided graph database results
- NEVER generate, fabricate, or guess requirement IDs, test case IDs, or component IDs not in the graph results
- If information is insufficient, clearly state "This information is not available in the database"
- Generating false information is strictly prohibited"""
        instruction = """Answer the question below based on the provided data.

**Answer Requirements**:
1. **Use ONLY information from "Graph Query Results"**
2. Cite only IDs explicitly present in graph results (never generate fake IDs like RQ-001)
3. **Include ALL requirement IDs without omission** (do not select only a few)
4. Explain major items in detail, list remaining items by category
5. If information is missing, state "Additional information not available in database"
6. Cite sources clearly (only actual document sections that exist)
7. Use markdown formatting (lists, tables, etc.)

**STRICTLY PROHIBITED**:
- Generating requirement/test case IDs not in graph results
- Answering based on guesses or general knowledge
- Creating placeholder or example data
"""

    prompt = f"""{instruction}

**Question**:
{user_question}

**Graph Query Results**:
{graph_context}

**Additional Context (from documents)**:
{vector_context if vector_context else "N/A"}

**Cypher Query Used**:
```cypher
{cypher_query}
```
"""

    # Add structured template for multiple results
    if graph_results and len(graph_results) > 0:
        # Collect all IDs by category
        all_ids_by_category = {}
        for record in graph_results:
            req_id = record.get("requirement_id")
            if req_id:
                # Extract category (e.g., FuncR, DesR, IntR, PerfR)
                category = req_id.split('_')[0] if '_' in req_id else "Other"
                if category not in all_ids_by_category:
                    all_ids_by_category[category] = []
                all_ids_by_category[category].append(req_id)

        if all_ids_by_category:
            num_results = len(graph_results)
            prompt += f"\n\n**ÌïÑÏàò ÎãµÎ≥Ä ÌòïÏãù** (Ï¥ù {num_results}Í∞ú Í≤∞Í≥º):\n"
            if num_results <= 5:
                prompt += "1. Í∞úÏöî: Ï¥ù Î™á Í∞úÏùò ÏöîÍµ¨ÏÇ¨Ìï≠Ïù¥ ÏòÅÌñ•ÏùÑ Î∞õÎäîÏßÄ Î™ÖÏãú\n"
                prompt += "2. Î™®Îì† ÏöîÍµ¨ÏÇ¨Ìï≠ÏùÑ ÏûêÏÑ∏Ìûà ÏÑ§Î™Ö\n"
            elif num_results <= 15:
                prompt += "1. Í∞úÏöî: Ï¥ù Î™á Í∞úÏùò ÏöîÍµ¨ÏÇ¨Ìï≠Ïù¥ ÏòÅÌñ•ÏùÑ Î∞õÎäîÏßÄ Î™ÖÏãú\n"
                prompt += "2. Ï£ºÏöî ÏöîÍµ¨ÏÇ¨Ìï≠ ÏÉÅÏÑ∏ ÏÑ§Î™Ö (5-7Í∞ú)\n"
                prompt += "3. Ï†ÑÏ≤¥ ÏöîÍµ¨ÏÇ¨Ìï≠ Î™©Î°ù (Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ)\n"
            else:
                prompt += "1. Í∞úÏöî: Ï¥ù Î™á Í∞úÏùò ÏöîÍµ¨ÏÇ¨Ìï≠Ïù¥ ÏòÅÌñ•ÏùÑ Î∞õÎäîÏßÄ Î™ÖÏãú\n"
                prompt += "2. Ï£ºÏöî ÏöîÍµ¨ÏÇ¨Ìï≠ ÏÉÅÏÑ∏ ÏÑ§Î™Ö (7-10Í∞ú)\n"
                prompt += "3. Ï†ÑÏ≤¥ ÏöîÍµ¨ÏÇ¨Ìï≠ Î™©Î°ù (Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ)\n"

            prompt += "\n**Í≤ÄÏ¶ùÏö© Ï†ÑÏ≤¥ ID Î™©Î°ù** (Î∞òÎìúÏãú Î™®Îëê Ìè¨Ìï®):\n"
            for category in sorted(all_ids_by_category.keys()):
                ids = all_ids_by_category[category]
                prompt += f"- {category}: {len(ids)}Í∞ú - {', '.join(sorted(ids))}\n"

            prompt += f"\n**Ìï©Í≥Ñ Í≤ÄÏ¶ù**: Î™®Îì† Ïπ¥ÌÖåÍ≥†Î¶¨ Í∞úÏàòÎ•º Ìï©ÏÇ∞ÌïòÎ©¥ Ï†ïÌôïÌûà {num_results}Í∞úÍ∞Ä ÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.\n"
            prompt += "\n**Ï§ëÏöî**: ÏúÑÏùò Ï†ÑÏ≤¥ ID Î™©Î°ùÏóê ÏûàÎäî Î™®Îì† ÏöîÍµ¨ÏÇ¨Ìï≠ÏùÑ Îπ†ÏßêÏóÜÏù¥ ÎãµÎ≥ÄÏóê Ìè¨Ìï®Ìï¥Ïïº Ìï©ÎãàÎã§.\n"

    prompt += "\nProvide a comprehensive answer:\n"

    # Call GPT-4
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,  # Slightly creative but mostly factual
            max_tokens=2000
        )

        if response and response.choices and len(response.choices) > 0:
            answer = response.choices[0].message.content.strip()
        else:
            logger.error("GPT-4 response is empty or invalid")
            answer = "Error: Empty response from GPT-4"

        # Extract citations
        citations = _extract_citations(graph_results, top_k_sections)

        return {
            "answer": answer,
            "citations": citations
        }

    except Exception as e:
        logger.error(f"Response synthesis failed: {e}", exc_info=True)
        return {
            "answer": f"Error generating response: {str(e)}",
            "citations": []
        }


def _synthesize_from_vectors(state: GraphRAGState, language: str) -> Dict[str, Any]:
    """
    Synthesize response from vector search results only.

    Args:
        state: GraphRAGState
        language: Target language (ko/en)

    Returns:
        Dict with 'answer' and 'citations'
    """
    user_question = state["user_question"]
    top_k_sections = state.get("top_k_sections", [])

    if not top_k_sections:
        return {
            "answer": "Í¥ÄÎ†® Î¨∏ÏÑúÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§." if language == "ko" else "No relevant documents found.",
            "citations": []
        }

    # Build context
    context = "\n\n---\n\n".join([
        f"**Source**: {sec['document']} - {sec['title']}\n\n{sec['content']}"
        for sec in top_k_sections[:5]
    ])

    # Language-specific prompts
    if language == "ko":
        system_prompt = "ÎãπÏã†ÏùÄ MOSAR ÏãúÏä§ÌÖú Í∏∞Ïà† Î¨∏ÏÑú Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. Ï†úÍ≥µÎêú Î¨∏ÏÑúÎ•º Î∞îÌÉïÏúºÎ°ú ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÌïòÏÑ∏Ïöî."
        instruction = "ÏïÑÎûò Î¨∏ÏÑúÎ•º Ï∞∏Í≥†ÌïòÏó¨ ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÌï¥Ï£ºÏÑ∏Ïöî. Ï∂úÏ≤òÎ•º Î™ÖÏãúÌïòÍ≥† ÎßàÌÅ¨Îã§Ïö¥ ÌòïÏãùÏúºÎ°ú ÏûëÏÑ±ÌïòÏÑ∏Ïöî."
    else:
        system_prompt = "You are a MOSAR system technical documentation expert. Answer questions based on provided documents."
        instruction = "Answer the question based on the documents below. Cite sources and use markdown formatting."

    prompt = f"""{instruction}

**Question**:
{user_question}

**Relevant Documents**:
{context}

Provide a comprehensive answer:
"""

    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=2000
        )

        if response and response.choices and len(response.choices) > 0:
            answer = response.choices[0].message.content.strip()
        else:
            logger.error("GPT-4 response is empty or invalid")
            answer = "Error: Empty response from GPT-4"

        citations = _extract_citations([], top_k_sections)

        return {
            "answer": answer,
            "citations": citations
        }

    except Exception as e:
        logger.error(f"Response synthesis failed: {e}", exc_info=True)
        return {
            "answer": f"Error: {str(e)}",
            "citations": []
        }


def _format_graph_results(graph_results: List[Dict[str, Any]]) -> str:
    """
    Format graph query results as readable text.

    Special handling for:
    - Decomposition tree (parent + descendants)
    - Traceability queries
    - General results

    Args:
        graph_results: List of Neo4j query results

    Returns:
        Formatted string
    """
    if not graph_results:
        return "No graph results available."

    # Check if this is a decomposition tree result
    if len(graph_results) == 1 and 'descendants' in graph_results[0]:
        return _format_decomposition_tree(graph_results[0])

    # Default: Convert to JSON for structured display
    formatted = json.dumps(graph_results, indent=2, ensure_ascii=False)

    # Truncate if too long
    max_chars = 8000
    if len(formatted) > max_chars:
        formatted = formatted[:max_chars] + "\n... [truncated]"

    return formatted


def _format_decomposition_tree(result: Dict[str, Any]) -> str:
    """
    Format requirements decomposition tree for clear visualization.

    Args:
        result: Single decomposition tree result

    Returns:
        Formatted tree structure as string
    """
    parent_id = result.get('parent_id')
    parent_statement = result.get('parent_statement', '')
    parent_type = result.get('parent_type', '')
    parent_level = result.get('parent_level', 'System')
    descendants = result.get('descendants', [])

    # Group descendants by level
    level1 = [d for d in descendants if d.get('level') == 1]
    level2 = [d for d in descendants if d.get('level') == 2]

    output = f"""
ÏöîÍµ¨ÏÇ¨Ìï≠ Î∂ÑÌï¥ Íµ¨Ï°∞ (Requirements Decomposition Tree)
=====================================================

üìã ÏÉÅÏúÑ ÏöîÍµ¨ÏÇ¨Ìï≠ (Parent Requirement)
  ID: {parent_id}
  Type: {parent_type}
  Level: {parent_level}
  Statement: {parent_statement[:200]}{'...' if len(parent_statement) > 200 else ''}

‚îî‚îÄ ÌïòÏúÑ ÏöîÍµ¨ÏÇ¨Ìï≠ ({len(level1)} Level-1 children, {len(level2)} Level-2 grandchildren)
"""

    # Level 1 children
    if level1:
        output += "\n   üìç Level 1 (Direct Children - Subsystem Level):\n"
        for i, child in enumerate(level1, 1):
            output += f"""
   {i}. {child.get('id')} ({child.get('type')})
      Statement: {child.get('statement', '')[:150]}{'...' if len(child.get('statement', '')) > 150 else ''}
      Verification: {child.get('verification', 'N/A')}
      Tests: {len(child.get('test_cases', []))} | Components: {len(child.get('components', []))}
"""

    # Level 2 grandchildren
    if level2:
        output += "\n   üìç Level 2 (Grandchildren - Component Level):\n"
        for i, gc in enumerate(level2, 1):
            test_info = f"{gc.get('test_count', 0)} tests: {gc.get('test_cases', [])}" if gc.get('test_count', 0) > 0 else "No tests"
            comp_info = f"{gc.get('component_count', 0)} components: {gc.get('components', [])}" if gc.get('component_count', 0) > 0 else "No components"

            output += f"""
   {i}. {gc.get('id')} ({gc.get('type')})
      Statement: {gc.get('statement', '')[:150]}{'...' if len(gc.get('statement', '')) > 150 else ''}
      Verification: {gc.get('verification', 'N/A')}
      {test_info}
      {comp_info}
"""

    # Summary statistics
    total_verified = sum(1 for d in descendants if d.get('test_count', 0) > 0)
    total_unverified = len(descendants) - total_verified

    output += f"""
üìä Í≤ÄÏ¶ù ÏÉÅÌÉú ÏöîÏïΩ (Verification Summary)
  - Ï¥ù ÌïòÏúÑ ÏöîÍµ¨ÏÇ¨Ìï≠: {len(descendants)}Í∞ú
  - ÌÖåÏä§Ìä∏ ÏôÑÎ£å: {total_verified}Í∞ú
  - ÎØ∏Í≤ÄÏ¶ù: {total_unverified}Í∞ú
  - Í≤ÄÏ¶ùÎ•†: {(total_verified/len(descendants)*100) if descendants else 0:.1f}%
"""

    return output


def _extract_citations(graph_results: List[Dict], sections: List[Dict]) -> List[Dict[str, str]]:
    """
    Extract citation information from results.

    IMPORTANT: Extract ALL citations from graph results to ensure completeness.

    Args:
        graph_results: Graph query results
        sections: Vector search sections

    Returns:
        List of citation dicts
    """
    citations = []

    # Citations from graph results - include ALL results (no limit)
    for result in graph_results:  # All results, not just [:5]
        if "requirement_id" in result:
            citations.append({
                "type": "requirement",
                "id": result["requirement_id"],
                "source": "SRD"
            })
        elif "component_id" in result:
            citations.append({
                "type": "component",
                "id": result["component_id"],
                "source": "MOSAR System"
            })

    # Citations from sections
    if sections:
        for sec in sections[:5]:
            if sec and isinstance(sec, dict):
                citations.append({
                    "type": "document_section",
                    "source": f"{sec.get('document', 'Unknown')} - {sec.get('title', 'Unknown')}",
                    "score": f"{sec.get('score', 0.0):.3f}"
                })

    return citations


# Standalone testing
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # Mock state with sample results
    state = GraphRAGState(
        user_question="What hardware handles network communication?",
        language="en",
        query_path=QueryPath.HYBRID,
        routing_confidence=0.85,
        matched_entities={"Component": ["R-ICU"]},
        top_k_sections=[
            {
                "section_id": "DDD-3.2",
                "title": "Network Architecture",
                "content": "The R-ICU is responsible for network communication using CAN and Ethernet protocols.",
                "document": "DDD",
                "doc_type": "detailed_design",
                "score": 0.89
            }
        ],
        extracted_entities={"Component": ["R-ICU"], "Protocol": ["CAN", "Ethernet"]},
        cypher_query="MATCH (c:Component {id: 'R-ICU'}) RETURN c",
        graph_results=[
            {
                "component_id": "R-ICU",
                "component_name": "Reduced Instrument Control Unit",
                "protocols": ["CAN", "Ethernet"],
                "related_requirements": ["FuncR_S110", "IntR_S102"]
            }
        ],
        final_answer="",
        citations=None,
        processing_time_ms=None,
        error=None
    )

    result = synthesize_response(state)

    print("=== Synthesized Response ===")
    print(result["final_answer"])
    print("\n=== Citations ===")
    print(json.dumps(result["citations"], indent=2))
