"""
Streaming Response Synthesis Node - Generate answers with real-time streaming

Provides streaming responses for better UX:
- Real-time token-by-token display
- Early user engagement (don't wait for full response)
- Chunked processing for long answers

Usage:
    # In LangGraph workflow
    workflow.add_node("synthesize_streaming", synthesize_response_streaming)

    # Or standalone
    for chunk in stream_synthesis(question, context):
        print(chunk, end='', flush=True)
"""

import logging
import os
import json
from typing import Dict, List, Any, Optional, Generator
from openai import OpenAI

from src.graphrag.state import GraphRAGState

logger = logging.getLogger(__name__)


def synthesize_response_streaming(state: GraphRAGState) -> GraphRAGState:
    """
    LangGraph Node: Synthesize final answer with streaming (non-blocking).

    NOTE: This node collects the full streamed response for LangGraph compatibility.
    For actual streaming to UI, use stream_synthesis() directly.

    Args:
        state: Current GraphRAGState

    Returns:
        Updated state with 'final_answer' and 'citations'
    """
    user_question = state["user_question"]
    language = state.get("language", "en")
    query_path = state.get("query_path")

    # Gather context
    context = _gather_context(state)

    logger.info(f"Synthesizing streaming response (language={language})")

    try:
        # Stream response and collect
        full_answer = ""
        citations = []

        for chunk in stream_synthesis(user_question, context, language, query_path):
            if isinstance(chunk, dict):
                # Metadata chunk (citations, etc.)
                if "citations" in chunk:
                    citations = chunk["citations"]
            else:
                # Text chunk
                full_answer += chunk

        # Update state
        state["final_answer"] = full_answer.strip()
        state["citations"] = citations

        logger.info(f"Streamed {len(full_answer)} characters")

    except Exception as e:
        logger.error(f"Streaming synthesis failed: {e}")
        state["final_answer"] = f"Error generating response: {str(e)}"
        state["citations"] = []
        state["error"] = str(e)

    return state


def stream_synthesis(
    user_question: str,
    context: Dict[str, Any],
    language: str = "en",
    query_path: Optional[str] = None
) -> Generator[str, None, None]:
    """
    Stream answer synthesis chunk by chunk.

    Args:
        user_question: User's question
        context: Context dict with vector_results, graph_results, etc.
        language: Language code
        query_path: Query execution path

    Yields:
        String chunks (text) or dict chunks (metadata)
    """
    # CRITICAL: Check for empty graph results (hallucination bug fix)
    graph_results = context.get("graph_results", [])
    graph_is_empty = not graph_results or len(graph_results) == 0

    vector_results = context.get("vector_results", [])
    vector_is_empty = not vector_results or len(vector_results) == 0

    # If query_path is pure_cypher AND graph results are empty, return clear message
    # For hybrid path, allow fallback to vector results if graph is empty
    if query_path == "pure_cypher" and graph_is_empty:
        logger.warning(f"Empty graph results for query: {user_question}")

        # Extract entity information for error message
        matched_entities = context.get("matched_entities", {})
        entity_info = ""
        if matched_entities:
            entity_list = []
            for entity_type, entity_data in matched_entities.items():
                if isinstance(entity_data, dict) and 'id' in entity_data:
                    entity_list.append(f"{entity_type}: {entity_data['id']}")
                elif isinstance(entity_data, list):
                    entity_list.extend([f"{entity_type}: {e}" for e in entity_data])
                elif isinstance(entity_data, str):
                    entity_list.append(f"{entity_type}: {entity_data}")
            if entity_list:
                entity_info = ", ".join(entity_list)

        # Yield empty results message
        if language == "ko":
            empty_message = f"""ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìš”ì²­í•˜ì‹  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ì¿¼ë¦¬ ì •ë³´**:
- ê²€ìƒ‰ ì—”í‹°í‹°: {entity_info if entity_info else 'ì•Œ ìˆ˜ ì—†ìŒ'}
- ì‹¤í–‰ëœ Cypher ì¿¼ë¦¬: ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë¨
- ë°˜í™˜ëœ ê²°ê³¼: 0ê°œ

**ê°€ëŠ¥í•œ ì›ì¸**:
1. ìš”ì²­í•˜ì‹  ì—”í‹°í‹°ê°€ ë°ì´í„°ë² ì´ìŠ¤ì— ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
2. ì—”í‹°í‹°ëŠ” ì¡´ìž¬í•˜ì§€ë§Œ ê´€ë ¨ ê´€ê³„(RELATES_TO, VERIFIES, USES_PROTOCOL ë“±)ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤
3. ë°ì´í„° ingestion ì¤‘ ì¼ë¶€ ê´€ê³„ê°€ ëˆ„ë½ë˜ì—ˆì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤

**ë„ì›€ë§**:
- ì—”í‹°í‹° ID í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš” (ì˜ˆ: FuncR_A101, R-ICU, CT-A-1)
- ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì‹œë„í•´ë³´ì„¸ìš”
- ë²¡í„° ê²€ìƒ‰ì„ í†µí•´ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•„ë³¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤"""
        else:
            empty_message = f"""No information found in the database for your query.

**Query Information**:
- Searched Entity: {entity_info if entity_info else 'Unknown'}
- Cypher Query: Successfully executed
- Results Returned: 0

**Possible Causes**:
1. The requested entity does not exist in the database
2. The entity exists but lacks relationships (RELATES_TO, VERIFIES, USES_PROTOCOL, etc.)
3. Some relationships may have been missed during data ingestion

**Suggestions**:
- Verify the entity ID format (e.g., FuncR_A101, R-ICU, CT-A-1)
- Try different search terms
- Use vector search to find related information in documents"""

        yield empty_message
        yield {"citations": []}  # Empty citations
        return  # Stop here, don't call LLM

    # For hybrid path, check if BOTH graph and vector results are empty
    if query_path == "hybrid" and graph_is_empty and vector_is_empty:
        logger.warning(f"Both graph and vector results are empty for query: {user_question}")

        if language == "ko":
            empty_message = """ë°ì´í„°ë² ì´ìŠ¤ì™€ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ì‹œë„í•œ ê²€ìƒ‰**:
- ì§€ì‹ ê·¸ëž˜í”„ ì¿¼ë¦¬: ê²°ê³¼ ì—†ìŒ
- ë¬¸ì„œ ë²¡í„° ê²€ìƒ‰: ìœ ì‚¬í•œ ë‚´ìš© ì—†ìŒ

**ì œì•ˆ**:
- ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë‚˜ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ë³´ì„¸ìš”
- ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ìž‘ì„±í•´ë³´ì„¸ìš”
- ì¡´ìž¬í•˜ëŠ” ì—”í‹°í‹° IDë¥¼ í™•ì¸í•˜ê³  ì§ˆë¬¸í•´ë³´ì„¸ìš” (ì˜ˆ: FuncR_A101, R-ICU)"""
        else:
            empty_message = """No relevant information found in database or documents.

**Search Attempted**:
- Knowledge graph query: No results
- Document vector search: No similar content

**Suggestions**:
- Try different keywords or search terms
- Make your question more specific
- Verify entity IDs exist and try again (e.g., FuncR_A101, R-ICU)"""

        yield empty_message
        yield {"citations": []}
        return

    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    model = os.getenv("LLM_MODEL", "gpt-4o")

    # Build prompt (pass query_path for context-aware system prompt)
    system_prompt = _build_system_prompt(language, query_path)
    user_prompt = _build_user_prompt(user_question, context, query_path)

    try:
        # Stream from OpenAI
        stream = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=2000,
            stream=True  # Enable streaming
        )

        # Yield chunks as they arrive
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                yield content

        # Yield citations as metadata (after streaming completes)
        citations = _extract_citations(context)
        if citations:
            yield {"citations": citations}

    except Exception as e:
        logger.error(f"Streaming failed: {e}")
        yield f"\n\n[Error: {str(e)}]"


def _gather_context(state: GraphRAGState) -> Dict[str, Any]:
    """
    Gather all context for synthesis.

    Args:
        state: Current state

    Returns:
        Context dict
    """
    return {
        "vector_results": state.get("top_k_sections", []),
        "graph_results": state.get("graph_results", []),
        "cypher_query": state.get("cypher_query"),
        "extracted_entities": state.get("extracted_entities", {}),
        "matched_entities": state.get("matched_entities", {}),
        "query_generation_method": state.get("query_generation_method")
    }


def _build_system_prompt(language: str, query_path: Optional[str] = None) -> str:
    """Build system prompt for synthesis.

    Args:
        language: Language code (ko/en)
        query_path: Query path (pure_cypher, hybrid, pure_vector)
    """
    # Pure Vector path uses different instructions (document-based search)
    if query_path == "pure_vector":
        if language == "ko":
            return """ë‹¹ì‹ ì€ MOSAR ìš°ì£¼ì„  ì‹œìŠ¤í…œì˜ ìš”êµ¬ì‚¬í•­ê³¼ ì„¤ê³„ ë¬¸ì„œ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.

**ë‹µë³€ ë°©ì‹**:
- ì œê³µëœ ë¬¸ì„œ ë‚´ìš©(Document Context)ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
- ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ì„¤ëª…í•˜ì„¸ìš”
- ê´€ë ¨ ì„¹ì…˜ì˜ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
- ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” "ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë°ížˆì„¸ìš”

ë‹µë³€ ê·œì¹™:
1. **ì •í™•ì„±**: ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©
2. **ì¢…í•©**: ì—¬ëŸ¬ ë¬¸ì„œ ì„¹ì…˜ì˜ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ì„¤ëª…
3. **ë§¥ë½**: ê° ì •ë³´ì˜ ì¶œì²˜(ë¬¸ì„œëª…, ì„¹ì…˜)ë¥¼ ì–¸ê¸‰
4. **êµ¬ì¡°í™”**: ëª…í™•í•œ êµ¬ì¡°ë¡œ ì •ë¦¬ (ê°œìš”, ìƒì„¸ ë‚´ìš©, ìš”ì•½)
5. **í•œê¸€**: ìžì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”"""
        else:
            return """You are an expert on MOSAR spacecraft system requirements and design documents.

**Answer Approach**:
- Base your answer on the provided Document Context
- Summarize and explain what is explicitly mentioned in the documents
- Synthesize information from multiple sections for comprehensive answers
- If information is not in the documents, state "This information is not found in the documents"

Answer Guidelines:
1. **Accuracy**: Use only what is explicitly stated in documents
2. **Synthesis**: Integrate information from multiple document sections
3. **Context**: Mention sources (document name, section) for each piece of information
4. **Structure**: Organize clearly (overview, details, summary)
5. **English**: Respond in clear, professional English"""

    # Pure Cypher / Hybrid paths use graph-based instructions
    if language == "ko":
        return """ë‹¹ì‹ ì€ MOSAR ìš°ì£¼ì„  ì‹œìŠ¤í…œì˜ ìš”êµ¬ì‚¬í•­ê³¼ ì„¤ê³„ ë¬¸ì„œ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.

**ì¤‘ìš” ì œì•½ì‚¬í•­**:
- ë°˜ë“œì‹œ ì œê³µëœ ê·¸ëž˜í”„ ë°ì´í„°ë² ì´ìŠ¤ ê²°ê³¼ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
- ê·¸ëž˜í”„ ê²°ê³¼ì— ì—†ëŠ” ìš”êµ¬ì‚¬í•­ ID, í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ID, ì»´í¬ë„ŒíŠ¸ IDë¥¼ ì ˆëŒ€ ìƒì„±í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
- ì •ë³´ê°€ ë¶ˆì¶©ë¶„í•˜ë©´ "ë°ì´í„°ë² ì´ìŠ¤ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…í™•ížˆ ë°ížˆì„¸ìš”
- í—ˆìœ„ ì •ë³´ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì€ ì—„ê²©ížˆ ê¸ˆì§€ë©ë‹ˆë‹¤

ë‹µë³€ ê·œì¹™:
1. **ì •í™•ì„±**: ì˜¤ì§ "Graph Database Results"ì— ìžˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©
2. **ID ì¸ìš©**: ê·¸ëž˜í”„ ê²°ê³¼ì— ëª…ì‹œëœ IDë§Œ ì¸ìš© (req1, TC1 ê°™ì€ ìž„ì˜ ID ìƒì„± ê¸ˆì§€)
3. **ì™„ì „ì„±**: ëª¨ë“  ìš”êµ¬ì‚¬í•­ IDë¥¼ ë¹ ì§ì—†ì´ í¬í•¨í•˜ì„¸ìš” (ì¼ë¶€ë§Œ ì„ íƒí•˜ì§€ ë§ ê²ƒ)
4. **êµ¬ì¡°í™”**:
   - ì£¼ìš” í•­ëª©ì€ ìžì„¸ížˆ ì„¤ëª…
   - ë‚˜ë¨¸ì§€ëŠ” ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë‚˜ì—´
   - bullet point ì‚¬ìš©
5. **ëª…í™•ì„±**: ê° ìš”êµ¬ì‚¬í•­ì˜ ì˜ë¯¸ì™€ ì¤‘ìš”ì„±ì„ ì„¤ëª…í•˜ì„¸ìš”
6. **í•œê¸€**: ìžì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”"""
    else:
        return """You are an expert on MOSAR spacecraft system requirements and design documents.

**CRITICAL CONSTRAINTS**:
- Answer ONLY using the provided graph database results
- NEVER generate, fabricate, or guess requirement IDs, test case IDs, or component IDs not in the graph results
- If information is insufficient, clearly state "This information is not available in the database"
- Generating false information is strictly prohibited

Answer Guidelines:
1. **Accuracy**: Use ONLY information from "Graph Database Results"
2. **Citations**: Cite only IDs explicitly in graph results (never generate fake IDs like req1, TC1)
3. **Completeness**: Include ALL requirement IDs without omission (do not select only a few)
4. **Structure**:
   - Explain major items in detail
   - List remaining items by category
   - Use bullet points
5. **Clarity**: Explain the meaning and importance of each requirement
6. **English**: Respond in clear, professional English"""


def _build_user_prompt(
    question: str,
    context: Dict[str, Any],
    query_path: Optional[str]
) -> str:
    """Build user prompt with context."""
    prompt_parts = []

    # Question
    prompt_parts.append(f"# Question\n{question}\n")

    # Graph results (most important) - Use formatted version for special queries
    if context.get("graph_results"):
        results = context["graph_results"]

        # Check if this is a decomposition tree or other special format
        if len(results) == 1 and 'descendants' in results[0]:
            # Use formatted tree output
            prompt_parts.append("# Graph Database Results (Decomposition Tree)")
            formatted = _format_graph_results(results)
            prompt_parts.append(formatted)
        else:
            # Default format for regular queries
            prompt_parts.append("# Graph Database Results")
            if len(results) > 20:
                prompt_parts.append(f"(Showing first 20 of {len(results)} results)\n")
                results = results[:20]

            for i, record in enumerate(results, 1):
                prompt_parts.append(f"\n## Result {i}")
                for key, value in record.items():
                    if value is not None:
                        # Format value nicely
                        if isinstance(value, list):
                            value_str = f"[{len(value)} items]" if len(value) > 5 else str(value)
                        else:
                            value_str = str(value)[:200]  # Truncate long strings
                        prompt_parts.append(f"- {key}: {value_str}")
        prompt_parts.append("")

    # Vector results (supplementary)
    if context.get("vector_results"):
        prompt_parts.append("# Document Context")
        for section in context["vector_results"][:3]:  # Top 3 only
            title = section.get("title", "Unknown")
            content_preview = section.get("content", "")[:300]
            prompt_parts.append(f"\n## {title}")
            prompt_parts.append(content_preview)
        prompt_parts.append("")

    # Cypher query (for transparency)
    if context.get("cypher_query"):
        prompt_parts.append("# Query Used")
        prompt_parts.append(f"```cypher\n{context['cypher_query']}\n```\n")

    # Task - with explicit structured template based on result count
    prompt_parts.append("# Task")
    prompt_parts.append("Answer the question based on the information above.")

    # Extract all requirement IDs from graph results for verification
    num_results = len(context.get("graph_results", []))
    if num_results > 0:
        # Collect all IDs by category
        all_ids_by_category = {}
        for record in context.get("graph_results", []):
            req_id = record.get("requirement_id")
            if req_id:
                # Extract category (e.g., FuncR, DesR, IntR, PerfR)
                category = req_id.split('_')[0] if '_' in req_id else "Other"
                if category not in all_ids_by_category:
                    all_ids_by_category[category] = []
                all_ids_by_category[category].append(req_id)

        # Build structured template
        if num_results <= 5:
            prompt_parts.append(f"\n**í•„ìˆ˜ ë‹µë³€ í˜•ì‹** (ì´ {num_results}ê°œ ê²°ê³¼):")
            prompt_parts.append("\n1. ê°œìš”: ì´ ëª‡ ê°œì˜ ìš”êµ¬ì‚¬í•­ì´ ì˜í–¥ì„ ë°›ëŠ”ì§€ ëª…ì‹œ")
            prompt_parts.append("2. ëª¨ë“  ìš”êµ¬ì‚¬í•­ì„ ìžì„¸ížˆ ì„¤ëª…")
        elif num_results <= 15:
            prompt_parts.append(f"\n**í•„ìˆ˜ ë‹µë³€ í˜•ì‹** (ì´ {num_results}ê°œ ê²°ê³¼):")
            prompt_parts.append("\n1. ê°œìš”: ì´ ëª‡ ê°œì˜ ìš”êµ¬ì‚¬í•­ì´ ì˜í–¥ì„ ë°›ëŠ”ì§€ ëª…ì‹œ")
            prompt_parts.append("2. ì£¼ìš” ìš”êµ¬ì‚¬í•­ ìƒì„¸ ì„¤ëª… (5-7ê°œ)")
            prompt_parts.append("3. ì „ì²´ ìš”êµ¬ì‚¬í•­ ëª©ë¡ (ì¹´í…Œê³ ë¦¬ë³„)")
        else:
            prompt_parts.append(f"\n**í•„ìˆ˜ ë‹µë³€ í˜•ì‹** (ì´ {num_results}ê°œ ê²°ê³¼):")
            prompt_parts.append("\n1. ê°œìš”: ì´ ëª‡ ê°œì˜ ìš”êµ¬ì‚¬í•­ì´ ì˜í–¥ì„ ë°›ëŠ”ì§€ ëª…ì‹œ")
            prompt_parts.append("2. ì£¼ìš” ìš”êµ¬ì‚¬í•­ ìƒì„¸ ì„¤ëª… (7-10ê°œ)")
            prompt_parts.append("3. ì „ì²´ ìš”êµ¬ì‚¬í•­ ëª©ë¡ (ì¹´í…Œê³ ë¦¬ë³„)")

        # Provide explicit ID list for verification
        if all_ids_by_category:
            prompt_parts.append(f"\n**ê²€ì¦ìš© ì „ì²´ ID ëª©ë¡** (ë°˜ë“œì‹œ ëª¨ë‘ í¬í•¨):")
            for category in sorted(all_ids_by_category.keys()):
                ids = all_ids_by_category[category]
                prompt_parts.append(f"- {category}: {len(ids)}ê°œ - {', '.join(sorted(ids))}")

            prompt_parts.append(f"\n**í•©ê³„ ê²€ì¦**: ëª¨ë“  ì¹´í…Œê³ ë¦¬ ê°œìˆ˜ë¥¼ í•©ì‚°í•˜ë©´ ì •í™•ížˆ {num_results}ê°œê°€ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

    prompt_parts.append("\n**ì¤‘ìš”**: ìœ„ì˜ ì „ì²´ ID ëª©ë¡ì— ìžˆëŠ” ëª¨ë“  ìš”êµ¬ì‚¬í•­ì„ ë¹ ì§ì—†ì´ ë‹µë³€ì— í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.")

    return "\n".join(prompt_parts)


def _format_graph_results(graph_results: List[Dict[str, Any]]) -> str:
    """
    Format graph query results as readable text.

    Special handling for:
    - Decomposition tree (parent + descendants)
    - Traceability queries
    - General results

    Args:
        graph_results: List of Neo4j query results

    Returns:
        Formatted string
    """
    if not graph_results:
        return "No graph results available."

    # Check if this is a decomposition tree result
    if len(graph_results) == 1 and 'descendants' in graph_results[0]:
        return _format_decomposition_tree(graph_results[0])

    # Default: Convert to JSON for structured display
    formatted = json.dumps(graph_results, indent=2, ensure_ascii=False)

    # Truncate if too long
    max_chars = 8000
    if len(formatted) > max_chars:
        formatted = formatted[:max_chars] + "\n... [truncated]"

    return formatted


def _format_decomposition_tree(result: Dict[str, Any]) -> str:
    """
    Format requirements decomposition tree for clear visualization.

    Args:
        result: Single decomposition tree result

    Returns:
        Formatted tree structure as string
    """
    parent_id = result.get('parent_id')
    parent_statement = result.get('parent_statement', '')
    parent_type = result.get('parent_type', '')
    parent_level = result.get('parent_level', 'System')
    descendants = result.get('descendants', [])

    # Group descendants by level
    level1 = [d for d in descendants if d.get('level') == 1]
    level2 = [d for d in descendants if d.get('level') == 2]

    output = f"""
ìš”êµ¬ì‚¬í•­ ë¶„í•´ êµ¬ì¡° (Requirements Decomposition Tree)
=====================================================

ðŸ“‹ ìƒìœ„ ìš”êµ¬ì‚¬í•­ (Parent Requirement)
  ID: {parent_id}
  Type: {parent_type}
  Level: {parent_level}
  Statement: {parent_statement[:200]}{'...' if len(parent_statement) > 200 else ''}

â””â”€ í•˜ìœ„ ìš”êµ¬ì‚¬í•­ ({len(level1)} Level-1 children, {len(level2)} Level-2 grandchildren)
"""

    # Level 1 children
    if level1:
        output += "\n   ðŸ“ Level 1 (Direct Children - Subsystem Level):\n"
        for i, child in enumerate(level1, 1):
            output += f"""
   {i}. {child.get('id')} ({child.get('type')})
      Statement: {child.get('statement', '')[:150]}{'...' if len(child.get('statement', '')) > 150 else ''}
      Verification: {child.get('verification', 'N/A')}
      Tests: {len(child.get('test_cases', []))} | Components: {len(child.get('components', []))}
"""

    # Level 2 grandchildren
    if level2:
        output += "\n   ðŸ“ Level 2 (Grandchildren - Component Level):\n"
        for i, gc in enumerate(level2, 1):
            test_info = f"{gc.get('test_count', 0)} tests: {gc.get('test_cases', [])}" if gc.get('test_count', 0) > 0 else "No tests"
            comp_info = f"{gc.get('component_count', 0)} components: {gc.get('components', [])}" if gc.get('component_count', 0) > 0 else "No components"

            output += f"""
   {i}. {gc.get('id')} ({gc.get('type')})
      Statement: {gc.get('statement', '')[:150]}{'...' if len(gc.get('statement', '')) > 150 else ''}
      Verification: {gc.get('verification', 'N/A')}
      {test_info}
      {comp_info}
"""

    # Summary statistics
    total_verified = sum(1 for d in descendants if d.get('test_count', 0) > 0)
    total_unverified = len(descendants) - total_verified

    output += f"""
ðŸ“Š ê²€ì¦ ìƒíƒœ ìš”ì•½ (Verification Summary)
  - ì´ í•˜ìœ„ ìš”êµ¬ì‚¬í•­: {len(descendants)}ê°œ
  - í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {total_verified}ê°œ
  - ë¯¸ê²€ì¦: {total_unverified}ê°œ
  - ê²€ì¦ë¥ : {(total_verified/len(descendants)*100) if descendants else 0:.1f}%
"""

    return output


def _extract_citations(context: Dict[str, Any]) -> List[str]:
    """Extract citation sources from context.

    IMPORTANT: Extract ALL citations from graph results to ensure completeness.
    """
    citations = []

    # From graph results - include ALL results (no limit)
    if context.get("graph_results"):
        for record in context["graph_results"]:  # All records, not just [:10]
            # Extract IDs
            for key, value in record.items():
                if 'id' in key.lower() and value:
                    if isinstance(value, str):
                        citations.append(value)
                    elif isinstance(value, list):
                        citations.extend([str(v) for v in value if v])

    # From vector results - use document name + section title for readability
    if context.get("vector_results"):
        for section in context["vector_results"][:5]:
            # Use document name and section title instead of technical section_id
            doc_name = section.get("document", "Unknown Document")
            doc_type = section.get("doc_type", "")
            section_title = section.get("title", "Untitled Section")

            # Format: "Document Type: Section Title" or "Document Name: Section Title"
            if doc_type:
                citation = f"{doc_type}: {section_title}"
            else:
                citation = f"{doc_name}: {section_title}"

            citations.append(citation)

    # Deduplicate but don't limit
    citations = list(dict.fromkeys(citations))  # Unique, no max limit

    return citations


# Standalone testing
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # Mock context
    test_context = {
        "graph_results": [
            {
                "requirement_id": "FuncR_S110",
                "requirement_statement": "The system shall support real-time communication",
                "components": ["R-ICU", "OBC"],
                "test_cases": ["TC_NET_001"]
            },
            {
                "requirement_id": "FuncR_S111",
                "requirement_statement": "The system shall use CAN bus protocol",
                "components": ["R-ICU"],
                "test_cases": []
            }
        ],
        "vector_results": [
            {
                "id": "DDD-4.1",
                "title": "Network Architecture",
                "content": "The R-ICU implements CAN bus communication at 1 Mbps..."
            }
        ],
        "cypher_query": "MATCH (c:Component {id: 'R-ICU'})<-[:RELATES_TO]-(req:Requirement) RETURN req"
    }

    print("="*80)
    print("Streaming Response Test")
    print("="*80)

    question = "What requirements are related to R-ICU?"

    print(f"\nQuestion: {question}\n")
    print("Streaming Answer:")
    print("-"*80)

    for chunk in stream_synthesis(question, test_context, language="en"):
        if isinstance(chunk, dict):
            print(f"\n\n[Citations: {chunk.get('citations', [])}]")
        else:
            print(chunk, end='', flush=True)

    print("\n" + "="*80)
